{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_covariance_matrix(n: int, shrinkage: Optional[float] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a random positive semi-definite covariance matrix of size n x n.\n",
    "    \n",
    "    If the optional `shrinkage` parameter is provided, the function applies a shrinkage \n",
    "    transformation to regularize the matrix. Shrinkage blends the covariance matrix \n",
    "    with a scaled identity matrix, which can help ensure positive semi-definiteness \n",
    "    and improve numerical stability in certain applications.\n",
    "\n",
    "    Args:\n",
    "        n (int): The size of the covariance matrix to generate.\n",
    "        shrinkage (Optional[float]): A value between 0 and 1 controlling the degree of shrinkage applied to the covariance matrix.\n",
    "            - If `shrinkage` is 0, no shrinkage is applied.\n",
    "            - If `shrinkage` is 1, the covariance matrix is replaced by a diagonal matrix scaled by its variances.\n",
    "            - If `shrinkage` is None, no shrinkage is applied unless the matrix is not positive semi-definite.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An n x n positive semi-definite covariance matrix.\n",
    "    \n",
    "    Raises:\n",
    "        AssertionError: If `shrinkage` is provided and is not in the range [0, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a random matrix\n",
    "    A = np.random.randn(n, n)\n",
    "    \n",
    "    # Construct a positive semi-definite matrix - A' * A ensures positive semi-definiteness\n",
    "    covariance_matrix = np.dot(np.transpose(A), A)\n",
    "\n",
    "    # Regularize/shrink if the matrix is not positive semi-definite\n",
    "    if shrinkage is not None or not all(np.linalg.eigvals(covariance_matrix) >= 0):\n",
    "\n",
    "        assert 0 <= shrinkage <= 1\n",
    "\n",
    "        # Lambda * Cov + (1 - Lambda) * I * Diagonal(Cov)\n",
    "        covariance_matrix = (1  - shrinkage) * covariance_matrix + shrinkage * np.multiply(np.diag(covariance_matrix), np.identity(n=n))\n",
    "    \n",
    "    # Normalize diagonal entries / adjust variances\n",
    "    for i in range(n):\n",
    "\n",
    "         # Ensure non-zero diagonal\n",
    "        covariance_matrix[i, i] = max(covariance_matrix[i, i], 1e-8) \n",
    "    \n",
    "    return covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume equal expected returns and a randomly generated covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.381507</td>\n",
       "      <td>0.052902</td>\n",
       "      <td>-0.460421</td>\n",
       "      <td>0.091174</td>\n",
       "      <td>0.044215</td>\n",
       "      <td>0.163740</td>\n",
       "      <td>0.053846</td>\n",
       "      <td>-0.065963</td>\n",
       "      <td>-0.195052</td>\n",
       "      <td>0.077731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052902</td>\n",
       "      <td>0.616898</td>\n",
       "      <td>-0.307003</td>\n",
       "      <td>-0.050129</td>\n",
       "      <td>0.050504</td>\n",
       "      <td>0.132555</td>\n",
       "      <td>0.061073</td>\n",
       "      <td>-0.120444</td>\n",
       "      <td>-0.101492</td>\n",
       "      <td>0.086012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.460421</td>\n",
       "      <td>-0.307003</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.052908</td>\n",
       "      <td>-0.170276</td>\n",
       "      <td>0.029537</td>\n",
       "      <td>-0.071896</td>\n",
       "      <td>-0.086830</td>\n",
       "      <td>0.460702</td>\n",
       "      <td>-0.020340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.091174</td>\n",
       "      <td>-0.050129</td>\n",
       "      <td>-0.052908</td>\n",
       "      <td>0.868746</td>\n",
       "      <td>-0.077121</td>\n",
       "      <td>-0.162184</td>\n",
       "      <td>-0.036259</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>-0.192100</td>\n",
       "      <td>-0.269480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.044215</td>\n",
       "      <td>0.050504</td>\n",
       "      <td>-0.170276</td>\n",
       "      <td>-0.077121</td>\n",
       "      <td>0.343049</td>\n",
       "      <td>-0.178416</td>\n",
       "      <td>0.089621</td>\n",
       "      <td>-0.028291</td>\n",
       "      <td>0.038680</td>\n",
       "      <td>0.107894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.163740</td>\n",
       "      <td>0.132555</td>\n",
       "      <td>0.029537</td>\n",
       "      <td>-0.162184</td>\n",
       "      <td>-0.178416</td>\n",
       "      <td>0.634229</td>\n",
       "      <td>-0.049367</td>\n",
       "      <td>-0.067953</td>\n",
       "      <td>0.042419</td>\n",
       "      <td>0.269888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.053846</td>\n",
       "      <td>0.061073</td>\n",
       "      <td>-0.071896</td>\n",
       "      <td>-0.036259</td>\n",
       "      <td>0.089621</td>\n",
       "      <td>-0.049367</td>\n",
       "      <td>0.852067</td>\n",
       "      <td>-0.037043</td>\n",
       "      <td>0.412994</td>\n",
       "      <td>0.080052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.065963</td>\n",
       "      <td>-0.120444</td>\n",
       "      <td>-0.086830</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>-0.028291</td>\n",
       "      <td>-0.067953</td>\n",
       "      <td>-0.037043</td>\n",
       "      <td>0.856126</td>\n",
       "      <td>0.083892</td>\n",
       "      <td>-0.156332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.195052</td>\n",
       "      <td>-0.101492</td>\n",
       "      <td>0.460702</td>\n",
       "      <td>-0.192100</td>\n",
       "      <td>0.038680</td>\n",
       "      <td>0.042419</td>\n",
       "      <td>0.412994</td>\n",
       "      <td>0.083892</td>\n",
       "      <td>0.677093</td>\n",
       "      <td>0.135250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.077731</td>\n",
       "      <td>0.086012</td>\n",
       "      <td>-0.020340</td>\n",
       "      <td>-0.269480</td>\n",
       "      <td>0.107894</td>\n",
       "      <td>0.269888</td>\n",
       "      <td>0.080052</td>\n",
       "      <td>-0.156332</td>\n",
       "      <td>0.135250</td>\n",
       "      <td>0.281433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.381507  0.052902 -0.460421  0.091174  0.044215  0.163740  0.053846   \n",
       "1  0.052902  0.616898 -0.307003 -0.050129  0.050504  0.132555  0.061073   \n",
       "2 -0.460421 -0.307003  1.000000 -0.052908 -0.170276  0.029537 -0.071896   \n",
       "3  0.091174 -0.050129 -0.052908  0.868746 -0.077121 -0.162184 -0.036259   \n",
       "4  0.044215  0.050504 -0.170276 -0.077121  0.343049 -0.178416  0.089621   \n",
       "5  0.163740  0.132555  0.029537 -0.162184 -0.178416  0.634229 -0.049367   \n",
       "6  0.053846  0.061073 -0.071896 -0.036259  0.089621 -0.049367  0.852067   \n",
       "7 -0.065963 -0.120444 -0.086830  0.192100 -0.028291 -0.067953 -0.037043   \n",
       "8 -0.195052 -0.101492  0.460702 -0.192100  0.038680  0.042419  0.412994   \n",
       "9  0.077731  0.086012 -0.020340 -0.269480  0.107894  0.269888  0.080052   \n",
       "\n",
       "          7         8         9  \n",
       "0 -0.065963 -0.195052  0.077731  \n",
       "1 -0.120444 -0.101492  0.086012  \n",
       "2 -0.086830  0.460702 -0.020340  \n",
       "3  0.192100 -0.192100 -0.269480  \n",
       "4 -0.028291  0.038680  0.107894  \n",
       "5 -0.067953  0.042419  0.269888  \n",
       "6 -0.037043  0.412994  0.080052  \n",
       "7  0.856126  0.083892 -0.156332  \n",
       "8  0.083892  0.677093  0.135250  \n",
       "9 -0.156332  0.135250  0.281433  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n: int = 10\n",
    "shrinkage: float = 0.00\n",
    "weight: np.ndarray = np.array([1 / n] * n)\n",
    "expected_return: np.ndarray = np.array([0.10] * n)\n",
    "\n",
    "covariance_matrix: np.ndarray = simulate_covariance_matrix(n=n, shrinkage=shrinkage)\n",
    "covariance_matrix /= np.max(np.diag(covariance_matrix))\n",
    "\n",
    "pd.DataFrame(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_optimal: np.ndarray = np.dot(np.linalg.inv(covariance_matrix), expected_return)\n",
    "w_optimal /= np.sum(w_optimal)\n",
    "np.sum(w_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61176594,  0.24430628,  0.47438002, -0.04228032, -0.50076447,\n",
       "       -0.70967591,  0.06675843,  0.26193796, -0.30043437,  0.89400645])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we scale the expected returns by any real number, this does not change our optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_return: np.ndarray = np.array([1.00] * n)\n",
    "w_optimal: np.ndarray = np.dot(np.linalg.inv(covariance_matrix), expected_return)\n",
    "w_optimal /= np.sum(w_optimal)\n",
    "np.sum(w_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61176594,  0.24430628,  0.47438002, -0.04228032, -0.50076447,\n",
       "       -0.70967591,  0.06675843,  0.26193796, -0.30043437,  0.89400645])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, if we assume they are all equal, we don't need to account for expected returns. Instead, our optimal weights simplify to inverse variance contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_return: np.ndarray = np.array([1.00] * n)\n",
    "w_optimal: np.ndarray = np.sum(np.linalg.inv(covariance_matrix), axis=1)\n",
    "w_optimal /= np.sum(w_optimal)\n",
    "np.sum(w_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61176594,  0.24430628,  0.47438002, -0.04228032, -0.50076447,\n",
       "       -0.70967591,  0.06675843,  0.26193796, -0.30043437,  0.89400645])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have omitted expected returns from our optimization process, something interesting happens in terms of our asset-level risk contribution... each asset contributes equally to our overall risk! This is definitionally risk parity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00011298, 0.00011298, 0.00011298, 0.00011298, 0.00011298,\n",
       "       0.00011298, 0.00011298, 0.00011298, 0.00011298, 0.00011298])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_contribution = np.dot(covariance_matrix, w_optimal)\n",
    "variance_contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if we revisit the first order condition to mean-variance optimization, we see the following:\n",
    "\n",
    "max w' er - lambda w' Sigma w\n",
    "\n",
    "FOC: \n",
    "er - lambda Sigma w = 0\n",
    "w* = lambda Sigma^-1 er\n",
    "\n",
    "If we omit expected returns or assume equal expected returns for each asset, we simplify to the following optimization problem:\n",
    "\n",
    "max - lambda w' Sigma w\n",
    "min lambda w' Sigma w\n",
    "s.t. w' Ones = 1\n",
    "\n",
    "This is definitionally a minimum variance optimization... this leads us to a very handy equivilance relation... risk parity IS the solution to the minimum variance optimization:\n",
    "\n",
    "w* = Sigma^-1 Ones / Ones' Sigma^-1 Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_min_variance_optimal: np.ndarray = np.dot(np.linalg.inv(covariance_matrix), np.ones(n)) / np.dot(np.transpose(np.ones(n)), np.dot(np.linalg.inv(covariance_matrix), np.ones(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61176594,  0.24430628,  0.47438002, -0.04228032, -0.50076447,\n",
       "       -0.70967591,  0.06675843,  0.26193796, -0.30043437,  0.89400645])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_min_variance_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a diagonal covariance matrix (i.e., all portfolio constituents are orthogonal/uncorrelated to each other), we can abstract away from the matrix form of the optimization. This simplifies to a simple inverse variance optimization using only the vector of diagonals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.741957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.803357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.772054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.138298</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.24511</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.381112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4    5        6         7  \\\n",
       "0  0.741957  0.000000  0.000000  0.000000  0.000000  0.0  0.00000  0.000000   \n",
       "1  0.000000  0.803357  0.000000  0.000000  0.000000  0.0  0.00000  0.000000   \n",
       "2  0.000000  0.000000  0.452875  0.000000  0.000000  0.0  0.00000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.695702  0.000000  0.0  0.00000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.772054  0.0  0.00000  0.000000   \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.000000  1.0  0.00000  0.000000   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.49715  0.000000   \n",
       "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.00000  0.138298   \n",
       "8  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.00000  0.000000   \n",
       "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.00000  0.000000   \n",
       "\n",
       "         8         9  \n",
       "0  0.00000  0.000000  \n",
       "1  0.00000  0.000000  \n",
       "2  0.00000  0.000000  \n",
       "3  0.00000  0.000000  \n",
       "4  0.00000  0.000000  \n",
       "5  0.00000  0.000000  \n",
       "6  0.00000  0.000000  \n",
       "7  0.00000  0.000000  \n",
       "8  0.24511  0.000000  \n",
       "9  0.00000  0.381112  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n: int = 10\n",
    "shrinkage: float = 1.00\n",
    "weight: np.ndarray = np.array([1 / n] * n)\n",
    "expected_return: np.ndarray = np.array([0.10] * n)\n",
    "\n",
    "covariance_matrix: np.ndarray = simulate_covariance_matrix(n=n, shrinkage=shrinkage)\n",
    "covariance_matrix /= np.max(np.diag(covariance_matrix))\n",
    "\n",
    "pd.DataFrame(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05505828, 0.05085024, 0.09020335, 0.05871891, 0.05291198,\n",
       "       0.04085088, 0.08217018, 0.29538386, 0.16666366, 0.10718863])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_return: np.ndarray = np.array([1.00] * n)\n",
    "w_optimal: np.ndarray = np.dot(np.linalg.inv(covariance_matrix), expected_return)\n",
    "w_optimal /= np.sum(w_optimal)\n",
    "w_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05505828, 0.05085024, 0.09020335, 0.05871891, 0.05291198,\n",
       "       0.04085088, 0.08217018, 0.29538386, 0.16666366, 0.10718863])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_optimal = (1 / np.diag(covariance_matrix)) / np.sum(1 / np.diag(covariance_matrix))\n",
    "w_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05505828, 0.05085024, 0.09020335, 0.05871891, 0.05291198,\n",
       "       0.04085088, 0.08217018, 0.29538386, 0.16666366, 0.10718863])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(covariance_matrix).dot(np.ones(n)) /np.ones(n).T.dot(np.linalg.inv(covariance_matrix).dot(np.ones(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05505828, 0.05085024, 0.09020335, 0.05871891, 0.05291198,\n",
       "       0.04085088, 0.08217018, 0.29538386, 0.16666366, 0.10718863])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.linalg.inv(covariance_matrix), axis=1) / np.sum(np.sum(np.linalg.inv(covariance_matrix), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This serves as a very powerful framwork to analyze seemingly simple, yet naunced problems. \n",
    "\n",
    "A prime example is the world of expected return agnostic portfolio management. For example, it is well known that the market-capitalization weighted benchmark is not very well risk-balanced and is highly concentrated in a few key risk and/or industry factors (e.g., size, growth, industry). A common alternative is the 1/N naive portolio that is completely agnostic to cross-sectional and individual asset risk properties. Neither of these is optimal from a diversification perspective, though. \n",
    "\n",
    "Assuming each stock equally contributes to the equity risk premium, we can omit asset-level returns from our portfolio optimization process. The PM can simply proceed as if the entire tradable universe has positive expectancy. This is desireable from a PM's perspective as expected returns are notoriously more difficult to predict than risk.  In this setting, it is adventageous to take advantage of the diversifying properties of non-perfectly correlated assets. One heuristic for this is equal industry exposure. Industries represent distinct sources of risk and return, and therefore, in expectation, provide diversifying properties to each other. Industry classifications are, however, only proxies, not definitions, of lowly correlated sources of risk. A more empirical, less fundamental, approach is equally weighted empirical correlation clusters. This serves as a more systematic, data-driven correlary to discretionary, and potentially imprecise, industry classiciations. A highly related, but slightly different, concept is principal component portfolios which directly orthogonalize latent sources of risk. Regardless of the approach, these serve as sufficient heuristics that, ex ante, provide more diversified exposures than both the market-capitalization and 1/N weighted portfolios. \n",
    "\n",
    "Despite the theoretical benefit of these heuristic approaches, they are strictly dominated by a process that explicitly seeks to maximize diversification. Risk parity, equal risk contribution, minimum variance, or inverse variance optimization procedures directly maximize the \"balance\" of risk across the key drivers of risk. These do not require potentially biased clustering algorithms or discretionary classification maps. This optimization procedure also explicitly controls for risk exposures, whereas the heuristic approaches only control for dollar exposures. This is not to say that you can't imagine a scenario where the heuristic clustering approach accounts for risk instead of dollar weight. In this scenario, the PM could assume his clusters are orthogonal and apply an inverse variance weighting scheme across his clusters. This is optimal if his constituents are clusters and the PM has no notion of each cluster's underlying constituents. However, this is not the case in reality, and the cluster-based approach potentially introduces less precision and suboptimal diversification. The PM is well aware of the full cross-section and can directly predict each stock's individual and cross-sectional risk properties with a tolerable degree of confidence and estimation error. Assuming the PM runs his book with an unbiased risk model, the asset-level (i.e., not cluster-level) risk parity approach is the most precise and theoretically optimal approach. \n",
    "\n",
    "Therefore, if the objective is to arrive at a risk-balanced portfolio that has meaningful exposure to the equity risk premium, the minimum variance / risk parity approach is optimal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate this behavior... center each stock's expected return around the ERP... randomize each stock's variance... each stock is uncorrelated... compare 1/N vs. Risk Parity / Min Variance optimization approaches... \n",
    "\n",
    "This will indicate that in the presence of unknown magnitude, but known directional expectancy, a risk parity optimization approach will likely dominate a 1/N or risk-imbalanced (e.g., market capitalization weighted) naive implementation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_risk_premium: float = 0.07\n",
    "expected_returns: np.ndarray = np.random.normal(loc=equity_risk_premium, scale=0.03, size=n)\n",
    "expected_variances: np.ndarray = np.random.randn(n) ** 2\n",
    "\n",
    "covariance_matrix: np.ndarray = np.multiply(expected_variances, np.identity(n))\n",
    "\n",
    "equal_weight: np.ndarray = np.array([1 / n] * n)\n",
    "minimum_variance_optimal_weight: np.ndarray = np.dot(np.linalg.inv(covariance_matrix), np.ones(n)) / np.dot(np.transpose(np.ones(n)), np.dot(np.linalg.inv(covariance_matrix), np.ones(n)))\n",
    "\n",
    "expected_sharpe_ratios: np.ndarray = expected_returns / np.sqrt(expected_variances)\n",
    "expected_equal_weight_sharpe_ratio: np.ndarray = np.dot(np.transpose(expected_returns), equal_weight) / np.sqrt(np.dot(np.dot(np.transpose(equal_weight), covariance_matrix), equal_weight))\n",
    "expected_minimum_variance_sharpe_ratio: np.ndarray = np.dot(np.transpose(expected_returns), minimum_variance_optimal_weight) / np.sqrt(np.dot(np.dot(np.transpose(minimum_variance_optimal_weight), covariance_matrix), minimum_variance_optimal_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'E[SR] of 1/N vs. Min Var / Risk Parity vs. Portfolio Constituents'}>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.Series({f\"E[SR {i}]\": sr for i, sr in enumerate(expected_sharpe_ratios)}),\n",
    "        pd.Series({\"E[1/N SR]\": expected_equal_weight_sharpe_ratio}),\n",
    "        pd.Series({\"E[Min Var SR]\": expected_minimum_variance_sharpe_ratio}),\n",
    "    ]\n",
    ").plot(kind=\"bar\", title=\"E[SR] of 1/N vs. Min Var / Risk Parity vs. Portfolio Constituents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_estate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
